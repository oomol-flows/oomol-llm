# LLM - OOMOL 平台的 AI 语言模型模块

一套即用型 AI 语言模型构建模块，专为 OOMOL 平台设计，帮助您无需编写代码即可创建智能对话工作流。

## 这个项目是什么？

本项目提供预构建的**模块**（可复用组件），让您可以将 AI 语言模型集成到 OOMOL 工作流中。可以将模块想象成乐高积木 - 您可以将它们连接在一起，构建强大的 AI 应用程序，如聊天机器人、翻译器、内容生成器等。

## 适合谁使用？

- **商业用户** - 希望自动化 AI 驱动的任务
- **内容创作者** - 寻求利用 AI 进行文本生成
- **产品经理** - 原型化 AI 功能
- **任何人** - 对无需编程构建 AI 工作流感兴趣

## 可用模块

### 1. LLM 模块
**功能说明：** 与 AI 语言模型通信，根据您的提示生成响应。

**使用场景：**
- 生成创意内容（故事、诗歌、文章）
- 回答任何主题的问题
- 转换或重新格式化文本
- 分析和总结信息

**主要特性：**
- 可自定义 AI 模型选择
- 可调节响应创造性（temperature）
- 支持多轮对话
- 连接失败时自动重试

### 2. LLM Chat 模块
**功能说明：** 专门用于构建具有历史消息记忆的对话体验。

**使用场景：**
- 构建客户支持聊天机器人
- 创建互动教学系统
- 开发对话助手
- 设计基于对话的应用程序

**主要特性：**
- 维护对话历史
- 支持系统指令来控制 AI 行为
- 实时流式响应
- 可配置超时和重试设置

### 3. Messages Generator 模块
**功能说明：** 使用模板为 AI 模型准备和格式化对话消息。

**使用场景：**
- 创建带有变量输入的动态提示
- 格式化多轮对话
- 构建可复用的提示模板
- 为 AI 处理准备结构化数据

**主要特性：**
- 基于模板的消息创建
- 支持 system、user 和 assistant 角色
- 变量替换（例如 `{{input}}`）
- 将现有对话与新消息组合

## 常见使用场景

### 💬 客户支持机器人
连接 **LLM Chat** 模块来处理客户咨询，提供上下文感知的响应。

### 🌐 翻译服务
使用带有翻译提示的 **LLM** 模块在不同语言之间转换文本。

### ✍️ 内容生成
组合 **Messages Generator** 和 **LLM** 模块来创建文章、电子邮件或社交媒体帖子。

### 📊 数据分析助手
将数据输入 **LLM** 模块以获取摘要、见解和建议。

## 快速开始

1. **安装包：** 将此 LLM 包导入您的 OOMOL 平台
2. **选择模块：** 选择适合您使用场景的模块
3. **配置设置：**
   - 选择您偏好的 AI 模型
   - 调整响应参数（temperature、max tokens）
   - 设置您的提示模板
4. **连接工作流：** 将模块链接在一起创建您的工作流
5. **测试与运行：** 执行您的工作流，看 AI 运行！

## 配置指南

### 模型选择
根据您的需求选择不同的 AI 模型：
- **deepseek-chat**（默认）：快速且经济实惠
- 模型选择器中提供其他支持的模型

### 响应控制
- **Temperature**（0-1）：控制创造性（0 = 专注，1 = 创意）
- **Top P**（0-1）：控制响应多样性
- **Max Tokens**：限制响应长度

### 重试与超时
- **Timeout**：响应的最大等待时间（默认：30 秒）
- **Retry Times**：失败时自动重试次数（默认：0）
- **Retry Sleep**：重试之间的延迟（默认：3.5 秒）

## 示例工作流

此项目在 `flows/` 目录中包含示例工作流：

- **chat**：交互式聊天机器人示例
- **translate**：语言翻译工作流
- **history**：带有上下文记忆的对话

## 支持与资源

- **代码仓库：** [https://github.com/oomol-flows/oomol-llm](https://github.com/oomol-flows/oomol-llm)
- **OOMOL 平台：** 在 [OOMOL 网站] 了解更多关于构建可视化工作流的信息

## 版本

当前版本：**0.3.5**

## 许可证

请查看代码仓库了解许可证信息。

---

**准备好构建您的第一个 AI 工作流了吗？** 将此包导入 OOMOL 并开始连接模块！
