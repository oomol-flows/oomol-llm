executor:
  name: python
  options:
    entry: __init__.py
inputs_def:
  - handle: messages
    json_schema:
      type: array
      items:
        type: object
        additionalProperties: false
        properties:
          role:
            enum:
              - system
              - user
              - assistant
            ui:options:
              labels:
                - System
                - User
                - Assistant
          content:
            type: string
    value:
    nullable: true
  - handle: timeout
    json_schema:
      type: number
      minimum: 0
    value: 30
  - handle: retry_times
    json_schema:
      type: integer
      minimum: 0
    value: 0
  - handle: retry_sleep
    json_schema:
      type: number
      minimum: 0
    value: 3.5
  - handle: stream
    json_schema:
      type: boolean
    value: true
  - handle: model
    json_schema:
      ui:options:
        title: Model
      ui:widget: self::model
    value:
      model: oomol-chat
      temperature: 0
      top_p: 0.5
      max_tokens: 4096
  - handle: template
    json_schema:
      anyOf:
        - type: string
        - type: array
          items:
            type: object
            additionalProperties: false
            properties:
              role:
                enum:
                  - system
                  - user
                  - assistant
                ui:options:
                  labels:
                    - System
                    - User
                    - Assistant
              content:
                type: string
      ui:options:
        title: Prompt
      ui:widget: self::messages
    value:
      - role: user
        content: ""
    schema_overrides:
      - schema:
          type: array
outputs_def:
  []
title: LLM
additional_inputs: true
additional_outputs: true
additional_outputs_def:
  - handle: output
    description: write your answer here
    json_schema:
      type: string
additional_inputs_def:
  []
